<!DOCTYPE html>
<html prefix="
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Machine Learning Geoscience · Machine Learning in 4D Seismic Inversion </title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="https://dramsch.net/phd/machine-learning-in-4d-seismic-inversion/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{equation}", "\\end{equation}"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><meta name="author" content="Jesper S Dramsch">
</head>
<body class="">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://dramsch.net/phd/">
                      <h1 id="brand"><a href="https://dramsch.net/phd/" title="Machine Learning Geoscience" rel="home">

        <span id="blog-title">Machine Learning Geoscience</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">Applications of Deep Neural Networks in 4D Seismic Data Analysis</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../abstract/">Abstract</a>
        <a class="sidebar-nav-item" href="../dansk-resume/">Dansk Resumé</a>
        <a class="sidebar-nav-item" href="../preface/">Preface</a>
        <a class="sidebar-nav-item" href="../introduction/">1: Introduction</a>
        <a class="sidebar-nav-item" href="../methods-theory/">2: Methods &amp; Theory</a>
        <a class="sidebar-nav-item" href="../unsupervised-geological-image-segmentation/">3: Unsupervised Geological Image Segmentation</a>
        <a class="sidebar-nav-item" href="../transfer-learning-in-automatic-seismic-interpretation/">4: Transfer learning in Automatic Seismic Interpretation</a>
        <a class="sidebar-nav-item" href="../complex-valued-neural-networks/">5: Complex-valued neural networks</a>
        <a class="sidebar-nav-item" href=".">6: Machine Learning in 4D Seismic Inversion</a>
        <a class="sidebar-nav-item" href="../3d-time-warping-for-4d-data/">7: 3D Time Warping for 4D Data</a>
        <a class="sidebar-nav-item" href="../conclusion/">8: Conclusion</a>
        <a class="sidebar-nav-item" href="../bibliography/">Bibliography</a>
        <a class="sidebar-nav-item" href="../acronyms/">Acronyms</a>
        <a class="sidebar-nav-item" href="../code/">Code</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2021         <a href="https://dramsch.net">Jesper S Dramsch</a>
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="post-title p-name"><a href="." class="u-url">Machine Learning in 4D Seismic Inversion</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 54%">
</colgroup>
<tbody>
<tr>
<td><p><a class="reference external" href="../2019.3.pdf"><img alt="image12" src="https://img.shields.io/badge/PDF-Download-important"></a></p></td>
<td><p><a class="reference external" href="https://github.com/JesperDramsch/4D-seismic-neural-inversion"><img alt="image13" src="https://img.shields.io/github/repo-size/JesperDramsch/4D-seismic-neural-inversion"></a></p></td>
<td><p><a class="reference external" href="https://doi.org/10.6084/m9.figshare.8218421.v1"><img alt="image14" src="https://img.shields.io/badge/talk-presentation-informational"></a></p></td>
<td><p><img alt="image15" src="https://img.shields.io/badge/license-Apache--2.0-green"></p></td>
</tr>
<tr>
<td colspan="4"><p><a class="reference external" href="https://orcid.org/0000-0001-8273-905X">Dramsch, J. S.</a>, Corte,
G., <a class="reference external" href="https://orcid.org/0000-0001-9588-6374">Amini, H.</a>, <a class="reference external" href="https://orcid.org/0000-0001-8593-3456">MacBeth,
C.</a>, &amp; <a class="reference external" href="https://orcid.org/0000-0003-2715-1653">Lüthje,
M.</a>. (2019). Including
Physics in Deep Learning–An example from 4D seismic pressure
saturation inversion. arXiv preprint arXiv:1904.02254.</p></td>
</tr>
<tr>
<td colspan="4"><p>Github: <a class="reference external" href="https://github.com/JesperDramsch/4D-seismic-neural-inversion">https://github.com/JesperDramsch/4D-seismic-neural-inversion</a></p></td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 54%">
</colgroup>
<tbody>
<tr>
<td><p><a class="reference external" href="../2019.2.pdf"><img alt="image16" src="https://img.shields.io/badge/PDF-Download-important"></a></p></td>
<td><p><a class="reference external" href="https://github.com/JesperDramsch/4D-seismic-neural-inversion"><img alt="image17" src="https://img.shields.io/github/repo-size/JesperDramsch/4D-seismic-neural-inversion"></a></p></td>
<td><p><a class="reference external" href="https://doi.org/10.6084/m9.figshare.7963775.v1"><img alt="image18" src="https://img.shields.io/badge/talk-presentation-informational"></a></p></td>
<td><p><a class="reference external" href="https://www.youtube.com/watch?v=-5YHV2vdKHo"><img alt="image19" src="https://img.shields.io/badge/video-youtube-red"></a></p></td>
</tr>
<tr>
<td colspan="4"><p><a class="reference external" href="https://orcid.org/0000-0001-8273-905X">Dramsch, J. S.</a>, Corte,
G., <a class="reference external" href="https://orcid.org/0000-0001-9588-6374">Amini, H.</a>, <a class="reference external" href="https://orcid.org/0000-0003-2715-1653">Lüthje,
M.</a>, &amp; <a class="reference external" href="https://orcid.org/0000-0001-8593-3456">MacBeth,
C.</a>. (2019, April). Deep
Learning Application for 4D Pressure Saturation Inversion Compared to
Bayesian Inversion on North Sea Data. In Second EAGE Workshop
Practical Reservoir Monitoring 2019.</p></td>
</tr>
<tr>
<td colspan="4"><p>Github: <a class="reference external" href="https://github.com/JesperDramsch/4D-seismic-neural-inversion">https://github.com/JesperDramsch/4D-seismic-neural-inversion</a></p></td>
</tr>
</tbody>
</table>
<hr class="docutils">
<p>This chapter discusses a neural network application to approximate the
pressure-saturation inversion of 4D seismic data. It contains two
workshop papers that discuss two different aspects of the construction
of the neural network architecture. Traditionally, 4D seismic qi often
relies on priors to reduce variance in the face of uncertainty. The
inversion problem in this chapter is a pressure-saturation inversion
from seismic amplitude difference maps in the Schiehallion field. The
first paper presents an ablation study of the components in the
architecture. The second paper discusses the neural network result and
presents a comparison to a classical Bayesian inversion.</p>
<div class="section" id="data">
<span id="data-1"></span><h2>Data</h2>
<p>The Schiehallion field is a stacked turbidite reservoir in the UK North
Sea, which makes it very heterogeneous and compartmentalized. The T31
sandstone reservoir has the most lateral extent with the thickness
ranging from 5 m to 30 m. The small thickness of the reservoir layer
results in the entire reservoir being contained in a single trough of a
seismic wavelet (<span class="math">\(\approx\frac{1}{2}\lambda\)</span>), which has
historically lead to applications using a 2D map view of the data. In
order to make the results comparable, we treat the network as a 2D map
instead of a 3D problem.</p>
<p>The data available consists of simulation and field data with several
years of collected seismic data. The baseline acquisition is from 1996
with additional time steps acquired in 1999, 2000, 2002, 2004, 2006,
2008, and 2010. There are simulation results and measured amplitude
difference maps. The simulated seismic data is based on pore volumes
from previous pore volume inversions, pressure changes and saturation
changes for water and gas. The ground truth pressure and saturation
changes are not available for validation of the field data directly,
which would be the ideal validation case.</p>
<p>Specifically, the seismic data consists of angle stacks in near, mid,
and far. The reflectivity of seismic data can be angle-dependent,
especially in the presence of fluids contained in the rock matrix.
(Castagna and Backus 1993). Angle stacks are constructed by selecting
subsets of the full dataset to average data within defined bands of
incidence angles. Commonly, angle stacks are constructed by stacking
over the offset hence avo. The process of stacking the data, despite
being partial stack increases the snr and is often necessary to obtain
reliable results in 4D qi.</p>
<p>The simulation results are noise-free calculations with only a single
simulation per year available. The recorded field seismic contains
significant levels of noise. The seismic field data can therefore
diverge from the theoretical prediction based on the pressure and
saturation data. These fluctuations are not smooth across individual
cells of the map, which can be seen in
<a class="reference external" href="#inv1:fig:schiehalliondata">15.7</a>.</p>
<p>The validation strategy in this problem setting is using one time step
as a hold-out set that is not used during the training of the neural
network. The time step used was recorded in the year 2004 and is
presented in <a class="reference external" href="#inv1:fig:schiehalliondata">15.7</a>. The remaining time
steps are used during the training. Results in the paper are presented
on the hold-out data.</p>
</div>
<div class="section" id="machine-learning-model">
<h2>Machine Learning Model</h2>
<p>A primary application of machine learning is building regression models. The data
available is not particularly abundant, which restricts the choice of
model or training strategy. Following a premise of simplicity, a dense
neural network was implemented, which treats each cell of a map
independently. It is possible that a convolutional neural network increases the performance, but
due to the nature of deep convolutional neural networks more training data needs to be generated.</p>
<p>In Jesper Sören Dramsch, Corte, et al. (2019d) we present a novel
network structure that explicitly includes avo gradient calculation
within the network as physical knowledge, shown in
<a class="reference external" href="#inv1:fig:avo-net">15.6</a>.</p>
<p>The network architecture was chosen to follow an encoder-decoder
architecture as a forcing function for information distillation. The
encoder decreases in size with each layer, gradually compressing the
input data, while the decoder decompresses the data to the designated
output (Dony and Haykin 1995). Conventionally, the middle layer is
called "bottleneck" or "code layer" as it contains the compressed
representation of the input data. Encoder-decoder architectures have
found wide application in neural network applications that necessitate
data transformation to a different representation (Worrall et al. 2017).</p>
<p>Additionally, the bottleneck layer is implemented as a variational
encoding layer to be less susceptible to noisy input. The specific
implementation is based on variational auto-encoders (Diederik P. Kingma
and Welling 2013). These replace the singular bottleneck layer with a
number of layers that represent the parameters in a parametric
probability distribution, most commonly the mean and variance of a
Gaussian distribution <span class="math">\(\mathcal{N}\left(\mu, \sigma\right)\)</span>. The
encoder then informs the Gaussian distribution at the bottleneck and the
decoder samples from the distribution during training. At inference,
these networks commonly return the mean of the distribution. Neural
networks are conventionally trained using stochastic gradient descent,
which is not well-behaved calculating the derivative of a random node.
Diederik P. Kingma and Welling (2013) popularized the
"reparameterization trick", which reformulates</p>
<div class="math">
\begin{equation*}
z \sim P_\phi (z|x),
\end{equation*}
</div>
<p>with <span class="math">\(z\)</span> being the bottleneck, <span class="math">\(P\)</span> being the probability of
the distribution <span class="math">\(\phi\)</span> to approximate, and <span class="math">\(x\)</span> being the
data sample to</p>
<div class="math">
\begin{equation*}
z = g(\phi, x, \epsilon)
\end{equation*}
</div>
<p>where <span class="math">\(g()\)</span> is the functional representation of <span class="math">\(\phi\)</span>
parameterized by <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span> for a Gaussian
distribution, and <span class="math">\(\epsilon\)</span> being a random sample from
<span class="math">\(\mathcal{N} (0,1)\)</span> that is the source of randomness in the
bottleneck layer computing as <span class="math">\(z = \mu + \sigma \cdot \epsilon\)</span>.</p>
<p>The pore volume is passed as-is to the network. The estimated pore
volume helps the network to decouple the rock matrix from the fluid
effects, which is further explored in <a class="reference external" href="#sec:confincluding">15.4</a>. A
schematic of the network is shown in <a class="reference external" href="#inv1:fig:avo-net">15.6</a>, which
shows the connections of the individual operations.</p>
<p>The network explicitly includes avo gradient calculation in the network
architecture, considering it is physical knowledge we know will
stabilize pressure and saturation change separation. Including basic
physics knowledge leads to the network learning residual information,
essentially defining another forcing function for the networks learning
process. The avo gradient can be calculated explicitly as input to the
network. However, performing the avo gradient calculation within the
network enables programmatic augmentation of the input data during
training. This implies that instead of learning one pre-computed avo
relation, we can perform data augmentation of the input data and train
on a significantly higher amount of correctly calculated avo gradients.
This strategy can significantly improve the training strategy.</p>
</div>
<div class="section" id="training-the-deep-neural-network-for-4d-seismic-inversion">
<h2>Training the Deep Neural Network for 4D Seismic Inversion</h2>
<p>The model training is carried out in multiple phases. The first phase
solely trains on un-augmented simulation data to determine an ideal
network structure. The second phase trains on the fixed architecture
with data augmentation to transfer the network to noisy field data. The
network is optimized on standard mse while monitoring the
R<sup>2</sup>-score.</p>
<p>The initial phase was carried out on simulation data with the data split
into one part for training and a separate data set for validation. The
seismic data from 2004 was held out as a test set. nas was applied to
the network to determine depth and width of the architecture, using a
tpe hyper-parameter search (J. Bergstra et al. 2015). This ensures an
architecture in a controlled test environment on simulation data that is
optimized for the complexity of the data.</p>
<p>In the second phase, to transfer the network to field data, the input of
the network was combined with additive Gaussian noise (Chris M. Bishop
1995) to train the network for noisy field data input. The noise level
was estimated in a manual process. Therefore, including the avo
calculation within the network forces the network to learn noisy avo
gradients that correspond to the augmented input. This process reduces
the R<sup>2</sup>-Score and mse, which is an expected effect of noisy
regression data (Hastie, Tibshirani, and Friedman 2009). Nevertheless,
this produces consistent results on field data upon visual inspection.</p>
<p>The paper in <a class="reference external" href="#sec:confincluding">15.4</a> provides an ablation study,
where parts of the neural network architecture are systematically
switched off. Ablation studies are commonly used to explore and evaluate
the effect of the individual components on the regression result. The
paper in <a class="reference external" href="#sec:conf4d">15.5</a> shows the results of the deep neural
network compared to a Bayesian inversion.</p>
</div>
<div class="section" id="workshop-paper-including-physics-in-deep-learning-an-example-from-4d-seismic-pressure-saturation-inversion">
<span id="sec-confincluding"></span><h2>Workshop Paper: Including Physics in Deep Learning – An example from 4D seismic pressure saturation inversion</h2>
<div class="section" id="introduction">
<span id="introduction-3"></span><h3>Introduction</h3>
<p>Physics in machine learning often relies on transformations of data to
beneficial domains and simulating additional data. Karpatne et al.
(2017) show a physics-guided approach to model lake temperatures with
neural networks. Schütt et al. (2017a) use deep neural networks to model
molecule energies and Oliveira, Paganini, and Nachman (2017) employ a
special architecture to capture scatter patterns in high-energy physics.
When building deep learning pipelines, we can make informed choices in
data modeling, but also build neural networks to maximize information
gain on the available data. Ulyanov, Vedaldi, and Lempitsky (2018) has
shown that the network architecture itself can be used as prior in
machine learning. These approaches translate well to geoscience, where
strong priors are often necessary to inform decisions.</p>
<p>Deep learning has revolutionized machine learning by replacing the
feature generation and augmentation step by learned internal
representations of features that maximize information gain. On image
data analysis of these neural network filters have shown close relations
to edge filters and color separators (Grün et al. 2016). Jesper Sören
Dramsch and Lüthje (2018b) have shown that these filters translate well
to seismic data. However, classic feed-forward neural networks do not
have the benefit of learning filters. However, these neural networks
benefit from recent improvements for regularization (Ioffe and Szegedy
2015), non-saturating and non-vanishing gradients (K. He et al. 2015),
and training on GPUs.</p>
<p>Neural networks for inversion of seismic data have a long history (Roeth
and Tarantola 1994). In (Jesper S. Dramsch et al. 2019) we show the
application of a deep multi-layer perceptron for map-based 4D seismic
pressure saturation inversion. In this work we show the information gain
of feed-forward multi-layer perceptron neural networks by including an
explicit calculation of the AVO gradient within the network
architecture. It’s exemplary for including domain knowledge as a prior
in machine learning.</p>
</div>
<div class="section" id="method">
<span id="method-1"></span><h3>Method</h3>
<p>We build a deep feed-forward network to invert seismic amplitude maps
for pressure and saturation changes. We use the high-level Python
framework <code class="docutils literal">keras</code> with a <code class="docutils literal">tensorflow</code> backend. The neural network
was trained on synthetic data, to subsequently predict field data. The
network takes the seismic input samplewise with near, mid, and far
stacks, and pore volume. We inject 20% Gaussian noise to model the
noisier field data directly after the input layer. This is fed to a
custom layer that calculates the PP AVO gradient between far-mid,
mid-near, and far-near. The main components are as follows:</p>
<div class="section" id="gaussian-noise-injection">
<h4>Gaussian noise injection</h4>
<p>The synthetic model is noise-free. While we get good results on the
training data and the modelled test data, the network does not transfer
well to noisy field data. Although the 4D NRMS is very low in the data
set, the sample-wise fluctuations in the field seismic differ
significantly from the synthetic data. We apply additive Gaussian noise
with <span class="math">\(\sigma = .02\)</span> to the seismic inputs separately to simulate
independent fluctuations of the seismic maps. This significantly
decreases the training and validation performance on noise free
synthetic data. On field data, however, this enables good transfer of
the neural network.</p>
<pre class="code python"><a name="rest_code_056b49d3efaa4c26b0258a3359dfbaf7-1"></a><span class="n">noisy_input</span> <span class="o">=</span> <span class="n">GaussianNoise</span><span class="p">(</span><span class="mf">0.02</span><span class="p">)(</span><span class="n">input_data</span><span class="p">)</span>
</pre>
</div>
<div class="section" id="explicit-avo-gradient-calculation">
<h4>Explicit AVO gradient calculation</h4>
<p>The Schiehallion field is a good example of imbalanced learning. We have
many samples of pressure changes <span class="math">\(\Delta P\)</span>, a good selection of
water saturation changes <span class="math">\(\Delta S_w\)</span>, and very few gas saturation
changes <span class="math">\(\Delta S_g\)</span>. Yet, the changes in gas saturation
<span class="math">\(\Delta S_g\)</span> produce the strongest changes in seismic P wave
amplitudes. Statistically, these can easily be regarded as outliers, and
therefore, possibly disregarded by the neural network. From decades of
seismic analysis, we know that the AVO gradient is very good for
pressure saturation separation. We implement an explicit calculation of
AVO gradients in the network.</p>
<div class="math">
\begin{equation*}
G = \frac{A_{\Theta_1} - A_{\Theta_0}}{x_{\Theta_1} - x_{\Theta_0}},
\end{equation*}
</div>
<p>where <span class="math">\(G\)</span> is the PP AVO gradient, <span class="math">\(A\)</span> is the seismic P wave
amplitude, <span class="math">\(x\)</span> is the offset, and <span class="math">\(\Theta\)</span> is the angle.</p>
<pre class="code python"><a name="rest_code_205564f84434479b87759a5e653c6a86-1"></a><span class="n">mid_near</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-2"></a>    <span class="k">lambda</span> <span class="n">inputs</span><span class="p">:</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-3"></a><span class="p">)([</span><span class="n">noisy_mid</span><span class="p">,</span> <span class="n">noisy_near</span><span class="p">])</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-4"></a>
<a name="rest_code_205564f84434479b87759a5e653c6a86-5"></a><span class="n">far_mid</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-6"></a>    <span class="k">lambda</span> <span class="n">inputs</span><span class="p">:</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-7"></a><span class="p">)([</span><span class="n">noisy_far</span><span class="p">,</span> <span class="n">noisy_mid</span><span class="p">])</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-8"></a>
<a name="rest_code_205564f84434479b87759a5e653c6a86-9"></a><span class="n">far_near</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-10"></a>    <span class="k">lambda</span> <span class="n">inputs</span><span class="p">:</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<a name="rest_code_205564f84434479b87759a5e653c6a86-11"></a><span class="p">)([</span><span class="n">noisy_far</span><span class="p">,</span> <span class="n">noisy_near</span><span class="p">])</span>
</pre>
</div>
<div class="section" id="encoder-decoder-architecture">
<h4>Encoder-decoder architecture</h4>
<p>Subsequently, the four input maps and the three gradient maps are
concatenated and fed to an encoder architecture that condenses the
information to an embedding layer <span class="math">\(z\)</span>. This layer learns a
collection of Gaussian distributions to represent the noisy input data
The decoder samples this variational embedding layer to calculate the
pressure change <span class="math">\(\Delta P\)</span>, change in water saturation
<span class="math">\(\Delta S_w\)</span>, and gas saturation <span class="math">\(\Delta S_g\)</span>.</p>
<p>The full architecture is of the encoder-decoder class. The encoder
reduces the number of parameters with each subsequent layer. This forces
the network to learn a lossy compression of the input data as
<span class="math">\(z\)</span>-vector. The decoder increases the number of nodes per layer
toward the output. The network therefore learns to correlate the low
resolution representation with the desired output.</p>
<div class="figure">
<img alt="Full Architecture from Jesper S. Dramsch et al. (2019)." id="inv2-fig-avonet" src="../images/AVO-Net.png" style="width: 110.0%;"><p class="caption">Full Architecture from Jesper S. Dramsch et al. (2019).</p>
</div>
</div>
<div class="section" id="variational-z-vector">
<h4>Variational Z Vector</h4>
<p>The inversion of noisy input benefits from a variational representation
of compressed z-vector. The networks learns Gaussian distributions in
the embedding layer. Therefore, we have to apply the reparametrization
trick outlined in Diederik P. Kingma and Welling (2013) to circumvent
the sampling process cannot be learned by gradient descent. We use the
implementation in Chollet and others (2015b) for variational
autoencoders.</p>
</div>
</div>
<div class="section" id="results">
<span id="results-2"></span><h3>Results</h3>
<div class="figure">
<img alt="Schiehallion 2004 Timestep Seismic data, pore volume and sim2seis results." id="inv2-fig-input" src="../images/x-seismic-input.png"><p class="caption">Schiehallion 2004 Timestep Seismic data, pore volume and sim2seis results.</p>
</div>
<p>In figure <a class="reference external" href="#inv2:fig:input">15.2</a> we show the 2004 time step of the
Schiehallion 4D. Figure <a class="reference external" href="#inv2:fig:vae">15.3</a> contains the inversion
result using the variational encoder decoder architecture. Some
coherency in the maps can be seen, but each map is very noisy and the
gas saturation map contains many data points that indicate gas
desaturation, which cannot be confirmed by production data.</p>
<div class="figure">
<img alt="Variational Encoder Decoder Architecture Inversion" id="inv2-fig-vae" src="../images/x-gustavonew-vae-alldata.png"><p class="caption">Variational Encoder Decoder Architecture Inversion</p>
</div>
<p>When we add the gradient, we can clean up some of the misfit in the gas
saturation maps <span class="math">\(\Delta S_g\)</span>. Particularly, the event with the
strongest softening in the amplitude maps, is partially reassigned to
the pressure map <span class="math">\(\Delta P\)</span>. However, the inversion process is
still very prone to noise. In figure <a class="reference external" href="#inv2:fig:noisegradvae">15.5</a>,
we show the inversion results of a AVO-gradient neural network with a
noise injection at training of <span class="math">\(\sigma = .02\)</span>. The inversion maps
are very coherent. Noise injection without gradient calculation does not
give adequate results.</p>
<div class="figure">
<img alt="AVO-Gradient Variational Encoder Decoder Architecture Inversion" id="inv2-fig-gradvae" src="../images/x-0-gradient-vae-noisy.png"><p class="caption">AVO-Gradient Variational Encoder Decoder Architecture Inversion</p>
</div>
<div class="figure">
<img alt="Noiseinjected AVO-Gradient Variational Encoder Decoder Architecture Inversion" id="inv2-fig-noisegradvae" src="../images/x-2-gradient-vae-noisy.png"><p class="caption">Noiseinjected AVO-Gradient Variational Encoder Decoder Architecture Inversion</p>
</div>
</div>
<div class="section" id="conclusions">
<span id="conclusions-1"></span><h3>Conclusions</h3>
<p>We have shown a neural network architecture that incorporates physical
domain knowledge to enable transfer from synthetic to field data. The
final inversion result has very good coherency, despite the network not
having any spatial context. While further investigation is necessary,
this indicates that useful information has been learned. This is one
example, where bias can be intentionally introduced into the network
architecture to include physics into machine learning.</p>
</div>
<div class="section" id="acknowledgements">
<span id="acknowledgements-2"></span><h3>Acknowledgements</h3>
<p>The research leading to these results has received funding from the
Danish Hydrocarbon Research and Technology Centre under the Advanced
Water Flooding program. We thank the sponsors of the Edinburgh
Time-Lapse Project, Phase VII (AkerBP, BP, CGG, Chevron, ConocoPhillips,
ENI, Equinor, ExxonMobil, Halliburton, Nexen, Norsar, OMV, Petrobras,
Shell, Taqa, and Woodside) for supporting this research. The Brazilian
governmental research-funding agency CNPq. We are also grateful to Linda
Hodgson and Ross Walder for important discussions on the field and
dataset.</p>
</div>
</div>
<div class="section" id="workshop-paper-deep-learning-application-for-4d-pressure-saturation-inversion-compared-to-bayesian-inversion-on-north-sea-data">
<span id="sec-conf4d"></span><h2>Workshop Paper: Deep Learning Application for 4D Pressure Saturation Inversion Compared to Bayesian Inversion on North Sea Data</h2>
<div class="section" id="id1">
<span id="introduction-4"></span><h3>Introduction</h3>
<p>Estimating reservoir property change during a period of production from
4D seismic data has been a concentrated challenge and ambition for
geoscientists in the oil and gas industry. These estimates can
contribute to a better history matching of the reservoir simulation and
for more comprehensive reservoir monitoring.</p>
<p>With the advance of machine learning techniques on all fronts in the
geosciences we can address what roles machine learning can take in the
established pressure and saturation inversion workflows and what other
new workflows can be constructed using this tool. Machine learning is
such a broad concept that it can be incorporated at different levels on
all the current well established workflows to diminish their weaknesses,
bringing more value to the pressure and saturation estimations from
seismic inversion. Not only that, with this tool we can create
completely new workflows that we are only beginning to grasp.</p>
<p>Here we will present results for two separate methodologies of seismic
inversion to changes in pressure and saturation. The first method is a
well established model-based Bayesian inversion method using a
calibrated petro-elastic model and convolution workflow as the forward
seismic modeling operator. In the second method we use a deep neural
network to model the inversion process, we use synthetic seismic data to
train the network, then apply the inversion to observed data. The
methods are applied to the same field data giving a nice platform to
compare the neural network inversion results to a more conventional
approach.</p>
</div>
<div class="section" id="schiehallion-data">
<h3>Schiehallion Data</h3>
<p>The inversions are applied to maps of Schiehallion’s upper T31
sandstone. It is a fairly thin reservoir (5-30m), which is well defined
in the seismic data by one single trough. For this reason, a map-based
approach is appropriate. Schiehallion is a highly compartmentalized
field with initial pressure close to bubble point pressure. Production
in this complex structure led to areas with strong pressurization due to
water injection into closed compartments, while other areas lack the
pressure support and experience gas release due to pressure depletion.
We face the challenge of inverting 4D seismic data to changes in
pressure, water saturation and gas saturation (<span class="math">\(\Delta\)</span>P,
<span class="math">\(\Delta\)</span>Sw and <span class="math">\(\Delta\)</span>Sg), so the methods need to deal
properly with the non-linearities due to each of these effects. The
seismic data analysed is a set of eight vintages (from 1996 to 2010).
These were reprocessed by CGG in 2014, following a 4D driven
multi-vintage workflow. The processing workflow was carefully optimized
to maintain 4D AVO amplitudes intact. Synthetic feasibility studies
showed that the 4D AVO attributes are in line with the theoretical
expectations. The seismic data used for inversion is the 4D difference
of the sum of negative amplitudes (<span class="math">\(\Delta\)</span>SNA) map attribute,
extracted from three angle-stacks, along the reservoir time window (see
figure <a class="reference external" href="#inv1:fig:schiehalliondata">15.7</a>).</p>
</div>
<div class="section" id="method-1-model-based-bayesian-inversion">
<h3>Method 1 - Model-based Bayesian inversion</h3>
<p>The Bayesian invesion workflow is explained in detail in Gustavo Corte,
MacBeth, and Amini (submitted 2019). Essentially the workflow uses a
petro-elastic model calibrated to the seismic data by H. Amini (2018)
and a convolutional step to model the seismic data. The
<span class="math">\(\Delta\)</span>SNA attribute is then extracted from the synthetic
seismic and compared to the real seismic <span class="math">\(\Delta\)</span>SNA map. Since
this is a map-based inversion, all realizations are sampled in map form
and then go through a conversion into the vertical reservoir simulation
grid in order to run the forward modelling process. We use a monte carlo
sampling algorithm to generate thousands of realizations of the full map
and from these extract best estimations and uncertainties. This
inversion is constructed in a Bayesian model-based form, with the
objective of bringing together information from the history matched
reservoir simulation and seismic data. Reservoir simulation results for
<span class="math">\(\Delta\)</span>P, <span class="math">\(\Delta\)</span>Sw and <span class="math">\(\Delta\)</span>Sg are
incorporated as prior knowledge, to settle ambiguities and lack of
seismic information. Where the seismic data lacks information about a
certain property the method will bring this information from the
simulation model. The inversion results will deviate from the simulation
in areas where the seismic data contains enough consistent information
to indicate an update is necessary.</p>
</div>
<div class="section" id="method-2-neural-network-inversion">
<h3>Method 2 - Neural network inversion</h3>
<p>We use a deep neural network to model the inversion process, based on
the synthetic convolution seismic data. Although convolutional neural
networks are considered the state of the art in spatially correlated
data, we show that a sample-wise feed forward neural network trained on
noise-free convolutional seismic can invert observed seismic data. We
aim to build a regression model that can invert physical seismic angle
stack data to pressure and saturation data.</p>
<p>Distinguishing pressure and saturation changes in 4D seismic data is a
hard to solve problem. In neural networks, this is no different. The
variation of data showing different pressure and saturation change
scenarios is sparse, which complicates training and may possibly be
disregarded as noise. This increases the need for training data
immensely. However, we can include prior physical insights into neural
networks to reduce the cost of training and uncertainty. As neural
networks are at its basis very large mathematical functions, we can
explicitly calculate the P-wave AVO gradient within the network to use
as additional information source, without the need of feeding it into
the network as input data. This has the added benefit of the network
learning on noisy gradients. The design choice for the neural networks
can be arbitrary, however, encoder-decoder networks have proven to force
neural networks to find meaningful relationships within the data and
reduce to these in the bottleneck or embedding layer. For the final
architecture we used <code class="docutils literal">hyperopt</code> (J. Bergstra, Yamins, and Cox 2013)
and <code class="docutils literal">keras</code> (Chollet and others 2015b). This allows us to use a Tree
of Parzen (TPE) estimator for hyperparameter estimation. The estimator
models <span class="math">\(P(x|y)\)</span> and <span class="math">\(P(y)\)</span>, where <span class="math">\(y\)</span> the quality of
fit and <span class="math">\(x\)</span> is the hyperparameter set drawn from a non-parametric
density (J. S. Bergstra et al. 2011).</p>
<div class="figure">
<img alt="Architecture for sample-based seismic inversion with explicit gradient calculation." id="inv1-fig-avo-net" src="../images/AVO-Net.png"><p class="caption">Architecture for sample-based seismic inversion with explicit gradient calculation.</p>
</div>
<p>The architecture is shown in figure <a class="reference external" href="#inv1:fig:avo-net">15.6</a>. Inputs
are Near, Mid, Far seismic, and Pore volume. These Input Layers are
passed on to calculate the mid-near, far-mid, and far-near gradients.
These four inputs and three gradients are concatenated and fed to the
encoder. z_mean and z_log_var build the variational embedding with
z_Lambda being the sampler fed to the decoder network. The decoder
splits into three output layers <span class="math">\(\Delta\)</span>P, <span class="math">\(\Delta\)</span>Sw,
and <span class="math">\(\Delta\)</span>Sg.</p>
<p>The network is trained using sim2seis results calculated for the seven
time-steps at seismic monitor acquisition times, it is then used to
invert each seismic monitor individually. The inversion results for the
synthetic data gave a consistent <span class="math">\(R^2\)</span>-score of over 0.6 for all
simultaneous inversion targets <span class="math">\(\Delta\)</span>P, <span class="math">\(\Delta\)</span>Sw and
<span class="math">\(\Delta\)</span>Sg with an encoder-decoder architecture and a
deterministic embedding layer. While we kept the main architecture
constant, we replaced the embedding layer with a variational formulation
to allow for noise in the input to output mapping added noise injection
to the input layer, to apply Gaussian Noise during the training phase.
This significantly improved the inference on observed seismic data. The
total training time for the network was 3 hours on a K5200 GPU,
prediction speed takes <span class="math">\(5.11~s \pm  22.1~ms\)</span>.</p>
</div>
<div class="section" id="schiehallion-field-data-example">
<h3>Schiehallion Field Data Example</h3>
<p>The field data differs significantly from the synthetic data in that it
is noisier, assuming the same ground truth. This is a true challenge for
a sample-wise process to produce consistent results. We have trained the
network with Gaussian noise on the input data with zero mean and a
standard deviation of <span class="math">\(\sigma = .02\)</span>, therefore, approximately
<span class="math">\(95~\%\)</span> of the noise may distort up to a maximum <span class="math">\(40~\%\)</span> of
the clean signal.</p>
<p>Figure <a class="reference external" href="#inv1:fig:schiehalliondata">15.7</a> shows the observed 4D
seismic maps (<span class="math">\(\Delta\)</span>SNA) for the 2004 monitor acquisition
using the 1996 acquisition as baseline.
Figure <a class="reference external" href="#inv1:fig:avo-net-results">15.8</a> shows, in the first row, the
simulation model results (used in the Bayesian method as prior
information), in the second row, the inversion results for the Bayesian
method, and in the third row, the inversion results for the neural
network method.</p>
<div class="figure">
<img alt="Schiehallion 2004 Timestep Seismic data, pore volume and sim2seis results." id="inv1-fig-schiehalliondata" src="../images/Seis_Data.PNG"><p class="caption">Schiehallion 2004 Timestep Seismic data, pore volume and sim2seis results.</p>
</div>
<p>From figure <a class="reference external" href="#inv1:fig:avo-net-results">15.8</a> we can see clearly the
influence of the prior simulation model in the Bayesian results. The
neural network does not use a prior, so the results are not influenced
by the simulation model and can be seen as a direct interpretation of
the seismic data. Comparing both we can see what bits of information the
Bayesian method is bringing from the prior. The seismic data is most
sensitive to gas saturation changes, so the Bayesian method is able to
capture this consistent information from seismic data and deviate
<span class="math">\(\Delta\)</span>Sg results from the initial prior. The results for gas
saturation are the most in agreement in both methods precisely because
all this information is coming from the seismic data. We see some
leakage of hardening effects into the <span class="math">\(\Delta\)</span>Sg results in
method 2 due to the fact that we cannot set constraints to that
inversion process. Since there is no initial gas saturation in those
areas the saturation change cannot be negative, these comprehensive
constraints are imbedded into the Bayesian workflow but not in the
neural network.</p>
<div class="figure">
<img alt="Schiehallion 2004 Timestep Bayesian Inversion and Neural Inversion" id="inv1-fig-avo-net-results" src="../images/NN_results.PNG"><p class="caption">Schiehallion 2004 Timestep Bayesian Inversion and Neural Inversion</p>
</div>
<p>Water saturation has a distinctive hardening effect on seismic data, but
in this map it is highly obscured by stronger overlying softening
effects due to pressure increase and gas breakout. The neural network
interprets all the hardening anomalies correctly as water saturation
increase, while controlling for noise in areas of softening amplitudes.
In those areas the seismic data does not contain useful information on
the water saturation so the Bayesian result relies on a strong prior to
compensate. All of the water saturation inverted by method 2 is in
agreement with method 1, but since method 1 has this additional
information from the prior, the map seems more coherent.</p>
<p>The pressure effect on seismic is highly non-linear. While high
increases in pressure show a very strong softening effect, milder
pressure variations (up to <span class="math">\(\pm7~MPa\)</span>) have very little influence
on the seismic data and are easily obscured by overlying effects. For
this reason, the neural network pressure inversion in regions of mild
change is low and often correlated with saturation. The Bayesian
inversion benefits from the prior to fill those pressure values. This
method does deviate from the prior in areas of strong softening signals
due to pressure increase, and those areas are also correctly interpreted
by the neural network inversion.</p>
<p>When we relax the prior of the Bayesian inversion, these results are
very noisy in the pressure and water saturation estimates. In these
areas the neural network inversion is robust to noise. During the neural
network training the pore volume has shown to be important in guiding
the inversion from the seismic data. Adding pore volume data adds a
structural component to the neural inversion process, which improves the
overall results from the sample-based method significantly.</p>
</div>
<div class="section" id="id2">
<span id="conclusions-2"></span><h3>Conclusions</h3>
<p>This work presents Deep Neural Inversion of 4D seismic data. We compare
the results with a Bayesian Inversion approach. We show that Deep Neural
Networks can model seismic inversion trained on synthetic data. Explicit
calculation of the P-wave AVO gradient within the network stabilizes the
pressure-saturation separation within the network and Noise Injection
enables the transfer to unseen seismic field data. Neural networks can
be an important tool to investigate nascent information in 4D seismic
data to improve inversion workflows and reduce uncertainty in seismic
analysis.</p>
<p>The Neural Inversion can be used as a valuable tool to explore purely
data-based inversion results in the presence of noise. It is able to
translate the ambiguous seismic amplitudes into much more easily
interpreted property maps. The value of the Bayesian inversion results
presented is in combining all knowledge about the reservoir to create a
general view of the reservoir dynamics. These results show the current
understanding of reservoir dynamics updated by imprinting seismic
information on top of the history matched simulation results.</p>
</div>
<div class="section" id="id3">
<span id="acknowledgements-3"></span><h3>Acknowledgements</h3>
<p>The research leading to these results has received funding from the
Danish Hydrocarbon Research and Technology Centre under the Advanced
Water Flooding program. We thank the sponsors of the Edinburgh
Time-Lapse Project, Phase VII (AkerBP, BP, CGG, Chevron, ConocoPhillips,
ENI, Equinor, ExxonMobil, Halliburton, Nexen, Norsar, OMV, Petrobras,
Shell, Taqa, and Woodside) for supporting this research. The Brazilian
governmental research-funding agency CNPq. We are also grateful to Linda
Hodgson and Ross Walder for important discussions on the field and
dataset. We thank Mikael Lüthje for valuable feedback.</p>
</div>
</div>
<div class="section" id="discussion-of-4d-inversion">
<h2>Discussion of 4D Inversion</h2>
<p>The workshop paper Jesper Sören Dramsch, Corte, et al. (2019a) contains
the neural network results compared to the simulation and Bayesian inversion
results, shown in <a class="reference external" href="#inv1:fig:avo-net-results">15.8</a>. This network
does not calculate the inversion solution; it merely approximates the
inverse problem. These initial results on limited training data show
that a neural network can estimate pressure saturation information from field data,
after training on simulation data.</p>
<p>The results presented in <a class="reference external" href="#inv1:fig:avo-net-results">15.8</a> contain
three indicators that the network learned a regression for the
Schiehallion field. The network returns the overall trend in increase
and decrease of pressure and saturation correctly. Additionally, the
range of output values for the network is unconstrained, but the network
calculates values in the ranges that are expected from the simulation
and Bayesian inversion results. However, and more interestingly, the
networks do not contain spatial information, being a feed-forward dnn
not a convolutional neural network, yet returns continuous albeit noisy outputs when re-assembled
into maps.</p>
<p>While the overall result is promising, regions of strong gas saturation
changes present a problem. This could be due to problems in the
modelling, as well as the fact, that they generate strong amplitude
differences and are far in between, essentially behaving like outliers.</p>
</div>
<div class="section" id="contribution-of-this-study">
<h2>Contribution of this study</h2>
<p>This study introduced a dnn to approximate a 4D qi pressure-saturation
inversion problem with a regression model. The contribution of this
study is threefold in that it approximated the pressure-saturation
inversion, included physical information in the network, and trained on
simulation data and transferred to field data. The work included in this
thesis are two workshop papers (Jesper Sören Dramsch, Corte, et al.
2019d, 2019a); however, a journal paper (Côrte et al. 2020) and
conference paper (G. Corte et al. 2020) have been published, resulting
directly from this work.</p>
</div>
</div>
    </div>
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{equation}", "\\end{equation}"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article>
</div>
            <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
